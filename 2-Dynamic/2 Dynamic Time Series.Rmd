---
title: "2: Dynamic Time Series"
subtitle: "Time Series and Panels"
author: "<large>J. Seawright</large>"
institute: "<small>Northwestern Political Science</small>" 
date: "`r format(Sys.Date(), '%A, %e %B, %Y')`"
output: 
  xaringan::moon_reader:
    css: xaringan-themer.css
     

---
class: center, middle

```{r, load_refs, include=FALSE, cache=FALSE}
# Initializes the bibliography
library(RefManageR)
BibOptions(check.entries = FALSE,
           bib.style = "authoryear", # Bibliography style
           max.names = 3, # Max author names displayed in bibliography
           sorting = "nyt", #Name, year, title sorting
           cite.style = "authoryear", # citation style
           style = "markdown",
           hyperlink = FALSE,
           dashed = FALSE)
#myBib <- ReadBib("assets/myBib.bib", check = FALSE)
# Note: don't forget to clear the knitr cache to account for changes in the
# bibliography.
library(DT)

ajgoogle <- read.csv("C:/Users/jnsno/OneDrive - Northwestern University/Seawright Teaching/TSCS Teaching/ajgoogle.csv")
```

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
style_mono_accent(
  base_color = "#1c5253",
  header_font_google = google_font("Josefin Sans"),
  text_font_google   = google_font("Montserrat", "300", "300i"),
  code_font_google   = google_font("Fira Mono"),
  text_font_size = "1.6rem"
)
```

---
class: center, middle

Here is a simple time-series model:

$$Y_{t} = \alpha + \beta X_{t} + \epsilon_{t}$$

---
class: center, middle

Suppose the error term has an $AR(1)$ process.

$$\epsilon_{t} = \rho \epsilon_{t-1} + \nu_{t}$$

---
class: center, middle

$$Y_{t} = \alpha + \beta X_{t} + \rho \epsilon_{t-1} + \nu_{t}$$

---
class: center, middle

$$\epsilon_{t-1} = Y_{t-1} - \alpha + \beta X_{t-1}$$

---
class: center, middle

$$Y_{t} = \alpha + \beta X_{t} + \rho (Y_{t-1} - \alpha + \beta X_{t-1}) + \nu_{t}$$
--

$$Y_{t} = (1 - \rho) \alpha + \rho Y_{t-1} + \beta X_{t} - \rho \beta X_{t-1} + \nu_{t}$$

---
class: center, middle

A more general time-series regression model:

$$Y_{t} = \alpha Y_{t-1} + \beta X_{t} + u_{t}$$

$$X_{t} = \rho X_{t-1} + e_{1,t}$$

$$u_{t} = \phi u_{t-1} + e_{2,t}$$
---
class: center, middle

Let's start simple: $\beta = \rho = \phi = 0$

--

$$Y_{t} = \alpha Y_{t-1} +  u_{t}$$

---
class: center, middle

Consider the following:

$$Y_{t} = \beta Y_{t-1} +  \epsilon_{t}$$

$$\epsilon_{t} = \rho \epsilon_{t-1} + \nu_{t}$$

---
#Assume
<br>
<br>
$$-1 < \beta < 1$$

$$-1 < \rho < 1$$

---

<br>
<br>

$$\rho Y_{t-1} = \rho \beta Y_{t-2} +  \rho \epsilon_{t-1}$$

$$\rho \epsilon_{t-1} = \rho Y_{t-1} - \rho \beta Y_{t-2}$$


---

<br>
<br>
$$Y_{t} = \beta Y_{t-1} +  \rho Y_{t-1} - \rho \beta Y_{t-2} + \nu_{t}$$

---
#The Workhorse Model

<br>
<br>

$$Y_{t} = \beta_{1} Y_{t-1} + \beta_{2} X_{t} +  \epsilon_{t}$$

$$\epsilon_{t} = \rho \epsilon_{t-1} + \nu_{t}$$

---
#The Workhorse Model

<br>
<br>

Keele and Kelly (2006) argue that the workhorse model should be estimated by OLS with a lagged dependent variable as long as:


- The dependent variable is **stationary.**
- The residuals are not strongly autocorrelated.

---
```{r, echo = TRUE}

for (i in 1:(nrow(ajgoogle)-1)) {
     ajgoogle$alexjoneslagged[i+1] <- ajgoogle$alexjones[i]
}

summary(lm(alexjones~alexjoneslagged, data=ajgoogle))
```

---
```{r, echo = TRUE}

for (i in 1:(nrow(ajgoogle)-2)) {
     ajgoogle$alexjoneslagged2[i+2] <- ajgoogle$alexjones[i]
}

summary(lm(alexjones~alexjoneslagged + alexjoneslagged2, data=ajgoogle))
```

---
```{r, echo = TRUE}

summary(lm(alexjones~alexjoneslagged + alexjoneslagged2 + donaldtrump + trumpapproval, data=ajgoogle))
```

---
```{r, echo = TRUE}

summary(lm(alexjones~alexjoneslagged + donaldtrump + trumpapproval, data=ajgoogle))
```

---
#Breusch-Godfrey Lagrange Multiplier Test

<br>
<br>

1. Estimate the model with OLS.
2. Regress residuals on the $X$ variables and lags of the residuals.
3. $(n-p) R^2$ converges to $\chi^{2}_{p}$.

---
```{r, echo = TRUE}

library(lmtest)

bgtest(alexjones~alexjoneslagged + alexjoneslagged2 + donaldtrump + trumpapproval, 
       data=ajgoogle, order=12, type="Chisq", fill=NA)
```

---

<br>
<br>

No significant results on BG?

-- Just use OLS.


---

<br>
<br>

Significant results on BG but the model doesn't include a lagged DV?

-- Use Cochrane-Orcutt.

1. Estimate with OLS
2. Regress residuals on lagged residuals to estimate serial correlation.
3. Subtract the estimated serial correlation out of the IVs and DVs.
4. Using the transformed IVs and DVs, repeat until convergence.

---
```{r, echo = TRUE}
library(orcutt)

aj.nolag <- lm(alexjones~donaldtrump + trumpapproval, data=ajgoogle)
aj.co <- cochrane.orcutt(aj.nolag)
summary(aj.co)
```
---

Significant results on BG and the model does include a lagged DV?

<br>
<br>
<br>

If we can get good estimates of the error term in our regression, we can figure out the autocorrelation and use GLS to get good estimates of the substantive parameters.

---

$$y_{t} = \beta y_{t-1} + \gamma x_{t} + \mu_{t}$$

1. Regress $y_{t-1}$ on $x_{t}$ and $x_{t-1}$. Save the fitted values.
2. Estimate the substantive regression of interest using the saved fitted values in place of the true values.
3. Form residuals with the coefficients from step 2 and the true values of $y$.
4. If the remaining residuals are serially correlated, use the serial correlation estimate to construct generalized differences with the original data and estimate the substantive model with those.

---
#Distributed Lags

<br>
<br>
<br>

$$y_{t} = \beta_{0} + \beta_{1} X_{t} + \beta_{2} X_{t-1} + \cdots + \epsilon_{t}$$


---
```{r, echo = FALSE, out.width="100%", fig.retina = 1}
library(knitr)
include_graphics("policyattitudesardl.jpg")
```


---
```{r, echo = FALSE}

ajgoogle2 <- read.csv("C:/Users/jnsno/OneDrive - Northwestern University/Seawright Teaching/TSCS Teaching/Spring 2023/2-Dynamic/alexjones2.csv")

library(dplyr)

ajgoogle2$X <- 1:nrow(ajgoogle2)

ajgoogle2$AlexJones <- lm(AlexJones ~ X + I(X^2) + I(X^3), data=ajgoogle2)$resid

ajgoogle2$alexjoneslagged <- NA

for (i in 1:(nrow(ajgoogle2)-1)) {
     ajgoogle2$alexjoneslagged[i+1] <- ajgoogle2$AlexJones[i]
}

ajgoogle2$alexjoneslagged2 <- NA

for (i in 1:(nrow(ajgoogle2)-2)) {
     ajgoogle2$alexjoneslagged2[i+2] <- ajgoogle2$AlexJones[i]
}
```
---
```{r, echo = TRUE}
summary(lm(AlexJones~alexjoneslagged + alexjoneslagged2 + SchoolShooting + GunControl + DonaldTrump, data=ajgoogle2))
```

---
```{r, echo = TRUE}
summary(lm(AlexJones~X + I(X^2) + I(X^3), data=ajgoogle2))
```


---
```{r, echo = TRUE}

bgtest(AlexJones~alexjoneslagged + alexjoneslagged2 + SchoolShooting + GunControl + DonaldTrump, 
       data=ajgoogle2, order=12, type="Chisq", fill=NA)
```

